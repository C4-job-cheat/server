"""
RAG ê¸°ë°˜ í•µì‹¬ì—­ëŸ‰ í‰ê°€ ì„œë¹„ìŠ¤ êµ¬í˜„.
Gemini ëª¨ë¸ê³¼ Firestore ë²¡í„° ì €ì¥ì†Œë¥¼ ì´ìš©í•´ í˜ë¥´ì†Œë‚˜ ì—­ëŸ‰ì„ ì ìˆ˜í™”í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import json
import logging
import re
from datetime import datetime
from typing import Any, Dict, Iterable, List, Optional

from .gemini_service import get_gemini_service
from .job_competencies import JobCompetenciesService
from .pinecone_service import get_pinecone_service, PineconeServiceError
from .cohere_service import get_cohere_service, CohereServiceError
from .pinecone_service import get_pinecone_service
from .cohere_service import get_cohere_service
from .firebase_personas import (
    PersonaNotFoundError,
    get_persona_document,
    mark_competency_evaluation,
)

logger = logging.getLogger(__name__)

COMPETENCY_EVALUATION_VERSION = "v2.0"
SYSTEM_PROMPT_VERSION = "v1.0"

SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ Job-Cheat í”Œë«í¼ì˜ ì¸ì‚¬ í‰ê°€ LLM ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

ì—­í• ê³¼ ê·œì¹™ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
- ì—­í• : í•œêµ­ì–´ ì±„ìš© ì‹œì¥ì„ ì´í•´í•˜ëŠ” HR ì „ë¬¸ê°€ë¡œì„œ, íŠ¹ì • ì§êµ°ì˜ í•µì‹¬ì—­ëŸ‰ì„ ì •ëŸ‰Â·ì •ì„± í‰ê°€í•©ë‹ˆë‹¤.
- ì„ë¬´: ì‹œìŠ¤í…œì´ ì œê³µí•œ ì§êµ°ë³„ í•µì‹¬ì—­ëŸ‰ ì •ì˜, ë²¡í„° ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸, ì‚¬ìš©ì ëŒ€í™”ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ê·¼ê±° ê¸°ë°˜ì˜ í‰ê°€ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- ì¶œë ¥: ì§€ì •ëœ JSON ìŠ¤í‚¤ë§ˆë¥¼ 100% ì¤€ìˆ˜í•˜ë©°, ë¶ˆí™•ì‹¤ì„±ì´ ìˆìœ¼ë©´ `confidence` ê°’ì„ ë‚®ê²Œ ì„¤ì •í•˜ê³  ê·¼ê±° ë¶€ì¡±ì„ ëª…ì‹œí•©ë‹ˆë‹¤.
- ì œí•œ: ì™¸ë¶€ ì§€ì‹ì´ë‚˜ ì¶”ì¸¡ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì œê³µëœ ë°ì´í„°ë§Œìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤. JSON ì™¸ì˜ ìì—°ì–´ ì„œìˆ ì€ ê¸ˆì§€ë©ë‹ˆë‹¤.

í•­ìƒ ìœ„ ì›ì¹™ì„ ì¤€ìˆ˜í•˜ë©° í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.
"""

COMPETENCY_EVALUATION_PROMPT = """
ë‹¹ì‹ ì€ ì „ë¬¸ ì¸ì‚¬ë‹´ë‹¹ìì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ì™€ ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • í•µì‹¬ì—­ëŸ‰ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

## í‰ê°€ ëŒ€ìƒ ì—­ëŸ‰
- ì—­ëŸ‰ ID: {competency_id}
- ì—­ëŸ‰ëª…: {competency_name}
- ì„¤ëª…: {competency_description}

## ì‚¬ìš©ì ê¸°ë³¸ ì •ë³´
- ì§êµ°: {job_category}
- ì§ë¬´: {job_role}
- ë³´ìœ  ê¸°ìˆ : {skills}
- ìê²©ì¦: {certifications}

## ì°¸ì¡°í•´ì•¼ í•  ë²¡í„° ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ (ê´€ë ¨ì„± ë†’ì€ ìˆœ)
{vector_contexts}

## ì‚¬ìš©ì ëŒ€í™” ê¸°ë¡ (ìµœëŒ€ {conversation_limit}ê±´)
{relevant_conversations}

## í‰ê°€ ê¸°ì¤€ (10ì  ë§Œì )
- 90-100ì : í•´ë‹¹ ì—­ëŸ‰ì—ì„œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤Œ
- 70-89ì : í•´ë‹¹ ì—­ëŸ‰ì—ì„œ ìˆ™ë ¨ëœ ìˆ˜ì¤€ì˜ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤Œ
- 50-69ì : í•´ë‹¹ ì—­ëŸ‰ì—ì„œ ê¸°ë³¸ì ì¸ ìˆ˜ì¤€ì˜ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤Œ
- 30-49ì : í•´ë‹¹ ì—­ëŸ‰ì—ì„œ ë¯¸ìˆ™í•œ ìˆ˜ì¤€ì˜ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤Œ
- 10-29ì : í•´ë‹¹ ì—­ëŸ‰ì—ì„œ ì´ˆë³´ ìˆ˜ì¤€ì˜ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤Œ

## ì‘ë‹µ í˜•ì‹ (JSON)
{{
  "score": ì •ìˆ˜ì ìˆ˜(1-100),
  "confidence": "ë§¤ìš°ë†’ìŒ|ë†’ìŒ|ë³´í†µ|ë‚®ìŒ|ë§¤ìš°ë‚®ìŒ ì¤‘ í•˜ë‚˜",
  "reasoning": "ì ìˆ˜ ê·¼ê±° ì„¤ëª…",
  "strong_signals": ["ê¸ì • ì‹ í˜¸ 1", "ê¸ì • ì‹ í˜¸ 2"],
  "risk_factors": ["ì£¼ì˜ ì‹ í˜¸ 1", "ì£¼ì˜ ì‹ í˜¸ 2"],
  "recommended_actions": ["ì¶”ì²œ ê°œì„  í–‰ë™ 1", "ì¶”ì²œ ê°œì„  í–‰ë™ 2"],
  "evidence": [
    {{
      "source": "vector_context" ë˜ëŠ” "conversation",
      "reference_id": "ì»¨í…ìŠ¤íŠ¸ ë˜ëŠ” ëŒ€í™” ID",
      "snippet": "ê´€ë ¨ ë¬¸ì¥",
      "relevance_score": 0.0ì—ì„œ 1.0 ì‚¬ì´ ìˆ˜ì¹˜
    }}
  ]
}}

ë°˜ë“œì‹œ JSONë§Œ ë°˜í™˜í•˜ê³ , ê°’ì´ ì—†ì„ ê²½ìš° í•´ë‹¹ ë°°ì—´ì„ ë¹ˆ ë°°ì—´ë¡œ ìœ ì§€í•´ì£¼ì„¸ìš”.
"""


class RAGCompetencyEvaluatorError(RuntimeError):
    """í•µì‹¬ì—­ëŸ‰ í‰ê°€ ê³¼ì •ì—ì„œ ë°œìƒí•œ ì˜ˆì™¸."""


class RAGCompetencyEvaluator:
    """RAG íŒŒì´í”„ë¼ì¸ì„ í†µí•´ í˜ë¥´ì†Œë‚˜ í•µì‹¬ì—­ëŸ‰ì„ í‰ê°€í•˜ëŠ” ì„œë¹„ìŠ¤."""

    def __init__(self) -> None:
        self.gemini_service = get_gemini_service()
        self.pinecone_service = get_pinecone_service()
        self.cohere_service = get_cohere_service()
        self.job_competencies = JobCompetenciesService()
        logger.info("RAG í•µì‹¬ì—­ëŸ‰ í‰ê°€ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    async def evaluate_persona_competencies(
        self,
        *,
        user_id: str,
        persona_id: str,
        top_k: int = 5,
        vector_context_top_k: int = 3,
        conversation_limit: int = 5,
    ) -> Dict[str, Any]:
        """íŠ¹ì • í˜ë¥´ì†Œë‚˜ì˜ í•µì‹¬ì—­ëŸ‰ì„ í‰ê°€í•˜ê³  Firestoreì— ê²°ê³¼ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤."""
        if not user_id:
            raise ValueError("user_idê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        if not persona_id:
            raise ValueError("persona_idê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        try:
            persona = get_persona_document(user_id=user_id, persona_id=persona_id)
        except PersonaNotFoundError:
            raise

        core_competencies = self._resolve_competencies(persona)
        if not core_competencies:
            raise RAGCompetencyEvaluatorError("í‰ê°€í•  í•µì‹¬ì—­ëŸ‰ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        evaluation_started_at = datetime.utcnow()
        competency_scores: Dict[str, int] = {}
        evaluation_details: List[Dict[str, Any]] = []
        evaluation_errors: List[Dict[str, Any]] = []

        for competency in core_competencies:
            competency_id = competency.get("id")
            competency_name = competency.get("name", "")
            competency_description = competency.get("description", "")
            query = self._build_query(competency_name, competency_description)

            try:
                search_results = await self._search_similar_conversations(
                    user_id=user_id,
                    query=query,
                    competency_id=competency_id,
                    top_k=top_k,
                )
            except Exception as exc:  # pragma: no cover - Pinecone/ì„ë² ë”© ì˜¤ë¥˜ ë˜í•‘
                logger.exception("ìœ ì‚¬ ëŒ€í™” ê²€ìƒ‰ ì‹¤íŒ¨")
                evaluation_errors.append(
                    {
                        "competency_id": competency_id,
                        "competency_name": competency_name,
                        "error": f"similarity_search_failed: {exc}",
                    }
                )
                search_results = []
            relevant_conversations = self._format_conversations(search_results, limit=conversation_limit)
            prompt = await self._generate_competency_prompt(
                persona=persona,
                competency=competency,
                relevant_conversations=relevant_conversations,
                vector_contexts=self._format_vector_contexts(search_results, limit=vector_context_top_k),
                conversation_limit=conversation_limit,
            )

            try:
                evaluation_response = await self._call_gemini_for_evaluation(prompt)
            except RAGCompetencyEvaluatorError as exc:
                evaluation_errors.append(
                    {
                        "competency_id": competency_id,
                        "competency_name": competency_name,
                        "error": f"llm_evaluation_failed: {exc}",
                    }
                )
                continue
            score = self._extract_score(evaluation_response)
            competency_scores[competency_id or competency_name] = score

            evaluation_details.append(
                {
                    "competency_id": competency_id,
                    "competency_name": competency_name,
                    "score": score,
                    "reasoning": evaluation_response.get("reasoning", ""),
                    "confidence": evaluation_response.get("confidence"),
                    "strong_signals": evaluation_response.get("strong_signals", []),
                    "risk_factors": evaluation_response.get("risk_factors", []),
                    "recommended_actions": evaluation_response.get("recommended_actions", []),
                    "evidence": evaluation_response.get("evidence", []),
                    "conversation_count": len(relevant_conversations),
                    "has_errors": False,
                }
            )

        evaluation_metadata = {
            "model": self.gemini_service.text_model,
            "system_prompt_version": SYSTEM_PROMPT_VERSION,
            "details": evaluation_details,
            "vector_store": {
                "backend": "pinecone",
                "namespace": user_id,
            },
        }

        updated_doc = mark_competency_evaluation(
            user_id=user_id,
            persona_id=persona_id,
            competency_scores=competency_scores,
            evaluation_version=COMPETENCY_EVALUATION_VERSION,
            evaluated_at=datetime.utcnow(),
            metadata=evaluation_metadata,
        )

        evaluation_completed_at = datetime.utcnow()

        return {
            "user_id": user_id,
            "persona_id": persona_id,
            "competency_scores": competency_scores,
            "evaluation_version": COMPETENCY_EVALUATION_VERSION,
            "evaluation_started_at": evaluation_started_at.isoformat() + "Z",
            "evaluation_completed_at": evaluation_completed_at.isoformat() + "Z",
            "details": evaluation_details,
            "errors": evaluation_errors,
            "persona_snapshot": updated_doc,
        }

    def _resolve_competencies(self, persona: Dict[str, Any]) -> List[Dict[str, Any]]:
        current = persona.get("core_competencies") or []
        if current:
            logger.info(f"ğŸ“‹ í˜ë¥´ì†Œë‚˜ì— ì €ì¥ëœ core_competencies ì‚¬ìš©: {len(current)}ê°œ")
            logger.info(f"   ğŸ“Š ì €ì¥ëœ ì—­ëŸ‰: {[comp.get('name', 'Unknown') for comp in current]}")
            return current

        job_category = persona.get("job_category", "")
        if not job_category:
            logger.warning("âŒ job_categoryê°€ ì—†ì–´ì„œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜")
            return []
        
        logger.info(f"ğŸ“¤ ì§êµ°ë³„ í•µì‹¬ì—­ëŸ‰ ì¡°íšŒ: {job_category}")
        competencies = self.job_competencies.get_core_competencies_by_job_category(job_category)
        logger.info(f"ğŸ“¥ ì§êµ°ë³„ í•µì‹¬ì—­ëŸ‰ ì¡°íšŒ ê²°ê³¼: {len(competencies)}ê°œ")
        logger.info(f"   ğŸ“Š ì¡°íšŒëœ ì—­ëŸ‰: {[comp.get('name', 'Unknown') for comp in competencies]}")
        return competencies

    def _build_query(self, name: str, description: str) -> str:
        parts = [part.strip() for part in [name, description] if part and part.strip()]
        return " - ".join(parts) if parts else "í•µì‹¬ì—­ëŸ‰"

    async def _search_similar_conversations(
        self,
        *,
        user_id: str,
        query: str,
        competency_id: Optional[str],
        top_k: int,
    ) -> List[Dict[str, Any]]:
        if not query:
            return []

        try:
            embeddings = await self.cohere_service.embed_texts(
                [query],
                model=self.cohere_service._default_model,
                input_type="search_query",
            )
        except CohereServiceError as exc:
            raise RAGCompetencyEvaluatorError(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {exc}") from exc

        if not embeddings:
            return []

        vector = embeddings[0]
        if not vector:
            return []
        filter_clause = None
        if competency_id:
            filter_clause = {"competency_tags": {"$contains": competency_id}}

        try:
            response = self.pinecone_service.query_similar(
                vector,
                namespace=user_id,
                top_k=top_k,
                include_metadata=True,
                filter=filter_clause,
            )
        except PineconeServiceError as exc:
            raise RAGCompetencyEvaluatorError(f"Pinecone ê²€ìƒ‰ ì‹¤íŒ¨: {exc}") from exc

        matches = response.get("matches", []) or []
        formatted: List[Dict[str, Any]] = []
        for match in matches:
            metadata = match.get("metadata", {}) or {}
            formatted.append(
                {
                    "conversation_id": match.get("id") or metadata.get("parent_conversation_id"),
                    "content": metadata.get("content", ""),
                    "score": float(match.get("score", 0.0)),
                    "competency_tags": metadata.get("competency_tags", []),
                }
            )
        return formatted

    def _format_vector_contexts(
        self,
        search_results: Iterable[Dict[str, Any]],
        *,
        limit: int,
    ) -> str:
        if not search_results:
            return "(ì°¸ì¡°í•  ë²¡í„° ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.)"

        formatted_lines: List[str] = []
        for idx, item in enumerate(search_results, start=1):
            if idx > max(0, limit):
                break
            formatted_lines.append(
                "- [{idx}] ì ìˆ˜={score:.3f}, ëŒ€í™”ID={cid}: {snippet}".format(
                    idx=idx,
                    score=float(item.get("score", 0.0)),
                    cid=item.get("conversation_id", "unknown"),
                    snippet=item.get("content", "").strip() or "(ë‚´ìš© ì—†ìŒ)",
                )
            )
        return "\n".join(formatted_lines)

    def _format_conversations(
        self,
        search_results: Iterable[Dict[str, Any]],
        *,
        limit: int,
    ) -> List[Dict[str, Any]]:
        formatted: List[Dict[str, Any]] = []
        for idx, item in enumerate(search_results, start=1):
            if idx > max(0, limit):
                break
            formatted.append(
                {
                    "conversation_id": item.get("conversation_id"),
                    "content": item.get("content", ""),
                    "score": float(item.get("score", 0.0)),
                }
            )
        return formatted

    async def _generate_competency_prompt(
        self,
        *,
        persona: Dict[str, Any],
        competency: Dict[str, Any],
        relevant_conversations: List[Dict[str, Any]],
        vector_contexts: str,
        conversation_limit: int,
    ) -> str:
        if not relevant_conversations:
            conversation_block = "(ê´€ë ¨ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œí•œì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.)"
        else:
            formatted = []
            for item in relevant_conversations:
                formatted.append(
                    "- ëŒ€í™”ID={conversation_id}, ìœ ì‚¬ë„={score:.3f}: {content}".format(
                        conversation_id=item.get("conversation_id", "unknown"),
                        score=float(item.get("score", 0.0)),
                        content=item.get("content", "").strip() or "(ë‚´ìš© ì—†ìŒ)",
                    )
                )
            conversation_block = "\n".join(formatted)

        return COMPETENCY_EVALUATION_PROMPT.format(
            competency_id=competency.get("id", ""),
            competency_name=competency.get("name", ""),
            competency_description=competency.get("description", ""),
            job_category=persona.get("job_category", ""),
            job_role=persona.get("job_role", ""),
            skills=", ".join(persona.get("skills", [])) or "(ì œê³µë˜ì§€ ì•ŠìŒ)",
            certifications=", ".join(persona.get("certifications", [])) or "(ì œê³µë˜ì§€ ì•ŠìŒ)",
            vector_contexts=vector_contexts,
            relevant_conversations=conversation_block,
            conversation_limit=max(0, conversation_limit),
        )

    async def _call_gemini_for_evaluation(self, prompt: str) -> Dict[str, Any]:
        try:
            full_prompt = f"{SYSTEM_PROMPT.strip()}\n\n{prompt.strip()}"
            response = await self.gemini_service.generate_structured_response(
                full_prompt,
                response_format="json",
            )
            if isinstance(response, dict):
                return response
            if isinstance(response, str):
                parsed = self._parse_llm_json(response)
                if parsed is not None:
                    return parsed
                return {"raw_response": response}
        except json.JSONDecodeError as exc:
            logger.warning("Gemini ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: %s", exc)
            return {"raw_response": response}
        except Exception as exc:  # pragma: no cover - Gemini í˜¸ì¶œ ì˜ˆì™¸ ë˜í•‘
            logger.error("Gemini í‰ê°€ í˜¸ì¶œ ì‹¤íŒ¨: %s", exc)
            raise RAGCompetencyEvaluatorError(f"Gemini í‰ê°€ í˜¸ì¶œ ì‹¤íŒ¨: {exc}") from exc

        return {"raw_response": response}

    def _parse_llm_json(self, raw: str) -> Optional[Dict[str, Any]]:
        if not raw or not raw.strip():
            return None

        candidate = raw.strip()

        fenced = re.match(r"```(?:json)?\s*(.*?)```", candidate, re.DOTALL)
        if fenced:
            candidate = fenced.group(1).strip()

        start = candidate.find("{")
        end = candidate.rfind("}")
        if start != -1 and end != -1 and end > start:
            candidate = candidate[start : end + 1]

        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            logger.warning("Gemini ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: ì›ë³¸ ì‘ë‹µ ì¼ë¶€=%s", candidate[:200])
            return None

    def _extract_score(self, evaluation_response: Dict[str, Any]) -> int:
        raw_score = evaluation_response.get("score")
        try:
            score = int(raw_score)
        except (TypeError, ValueError):
            score = 0
        score = max(1, min(100, score)) if score else 50
        return score


_evaluator_instance: Optional[RAGCompetencyEvaluator] = None


def get_competency_evaluator() -> RAGCompetencyEvaluator:
    """í•µì‹¬ì—­ëŸ‰ í‰ê°€ ì„œë¹„ìŠ¤ ì‹±ê¸€í„´ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _evaluator_instance
    if _evaluator_instance is None:
        _evaluator_instance = RAGCompetencyEvaluator()
        logger.info("RAG í•µì‹¬ì—­ëŸ‰ í‰ê°€ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±")
    return _evaluator_instance
