#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ëŒ€í™” ë‚´ì—­ RAG ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤

ì´ ëª¨ë“ˆì€ ChatGPT ëŒ€í™” ë‚´ì—­ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ê³ ,
Firestoreì— ì €ì¥í•œ í›„ Pineconeì— ì„ë² ë”©í•˜ì—¬ RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime


from .cohere_service import CohereService
from .pinecone_service import PineconeService
from .firebase_storage import download_persona_json

logger = logging.getLogger(__name__)


class ConversationRAGServiceError(RuntimeError):
    """ëŒ€í™” RAG ì„œë¹„ìŠ¤ ê´€ë ¨ ì˜ˆì™¸."""


class ConversationRAGService:
    """ëŒ€í™” ë‚´ì—­ RAG ì‹œìŠ¤í…œì„ ê´€ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤."""
    
    def __init__(self):
        """ì„œë¹„ìŠ¤ ì´ˆê¸°í™”."""
        self.cohere_service = CohereService()
        self.pinecone_service = PineconeService()
    
    def process_conversation_json(self, user_id: str, document_id: str) -> List[Dict[str, Any]]:
        """
        Firebase Storageì—ì„œ JSON íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ëŒ€í™” ë‚´ì—­ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            user_id: ì‚¬ìš©ì ID
            document_id: ë¬¸ì„œ ID (JSON íŒŒì¼ ID)
            
        Returns:
            ì²­í¬ ë”•ì…”ë„ˆë¦¬ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            
        Raises:
            ConversationRAGServiceError: ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ
        """
        try:
            logger.info(f"ëŒ€í™” JSON íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: user_id={user_id}, document_id={document_id}")
            
            # Firebase Storageì—ì„œ JSON íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            result = download_persona_json(
                user_id=user_id,
                document_id=document_id
            )
            
            if not result["exists"]:
                raise ConversationRAGServiceError(f"JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {result['path']}")
            
            # JSON ë‚´ìš© íŒŒì‹±
            json_content = result["content"]
            data = json.loads(json_content)
            
            chunks = []
            chunk_index = 0
            
            # ëŒ€í™” ë‚´ì—­ ìˆœíšŒ
            conversations = data.get('conversations', [])
            for conv in conversations:
                messages = conv.get('messages', [])
                
                for i, message in enumerate(messages):
                    role = message.get('role')
                    text = message.get('content', '')
                    
                    if not text.strip():
                        continue
                    
                    # ì²­í¬ ìƒì„±
                    chunk = {
                        'chunk_id': f"{document_id}-{chunk_index}",
                        'role': role,
                        'text': text,
                        'document_id': document_id,
                        'conversation_id': conv.get('conversation_id'),
                        'conversation_title': conv.get('title'),
                        'timestamp': datetime.utcnow().isoformat(),
                        'chunk_index': chunk_index
                    }
                    
                    chunks.append(chunk)
                    chunk_index += 1
            
            logger.info(f"ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ: ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            return chunks
            
        except Exception as exc:
            logger.error(f"ëŒ€í™” JSON ì²˜ë¦¬ ì‹¤íŒ¨: {exc}")
            raise ConversationRAGServiceError(f"ëŒ€í™” JSON ì²˜ë¦¬ ì‹¤íŒ¨: {exc}") from exc
    
    def _filter_vectors_by_metadata_size(self, vectors: List[Dict[str, Any]], user_id: str) -> List[Dict[str, Any]]:
        """
        ë©”íƒ€ë°ì´í„° í¬ê¸° ì œí•œ(40KB)ì„ ì´ˆê³¼í•˜ëŠ” ë²¡í„°ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
        í¬ê¸°ë¥¼ ì´ˆê³¼í•  ê²½ìš° ëŒ€í™” ë‚´ì—­ì„ ì¤„ì—¬ê°€ë©´ì„œ ì¬ì‹œë„í•©ë‹ˆë‹¤.
        
        Args:
            vectors: ì²˜ë¦¬í•  ë²¡í„° ë¦¬ìŠ¤íŠ¸
            user_id: ì‚¬ìš©ì ID (ë¡œê¹…ìš©)
            
        Returns:
            í•„í„°ë§ëœ ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        import json
        
        # Pinecone ë©”íƒ€ë°ì´í„° í¬ê¸° ì œí•œ (40KB)
        MAX_METADATA_SIZE = 40 * 1024  # 40KB
        
        filtered_vectors = []
        skipped_count = 0
        
        for vector in vectors:
            try:
                # ë©”íƒ€ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì§ë ¬í™”í•˜ì—¬ í¬ê¸° í™•ì¸
                metadata_json = json.dumps(vector['metadata'], ensure_ascii=False)
                metadata_size = len(metadata_json.encode('utf-8'))
                
                if metadata_size <= MAX_METADATA_SIZE:
                    filtered_vectors.append(vector)
                else:
                    # ë©”íƒ€ë°ì´í„° í¬ê¸°ê°€ ì´ˆê³¼í•˜ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„
                    logger.warning(f"ë©”íƒ€ë°ì´í„° í¬ê¸° ì´ˆê³¼: {metadata_size} bytes > {MAX_METADATA_SIZE} bytes")
                    logger.warning(f"ë²¡í„° ID: {vector['id']}")
                    
                    # í…ìŠ¤íŠ¸ë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„
                    reduced_vector = self._reduce_metadata_size(vector, MAX_METADATA_SIZE)
                    if reduced_vector:
                        filtered_vectors.append(reduced_vector)
                        logger.info(f"ë©”íƒ€ë°ì´í„° í¬ê¸° ì¶•ì†Œ ì„±ê³µ: {vector['id']}")
                    else:
                        skipped_count += 1
                        logger.warning(f"ë©”íƒ€ë°ì´í„° í¬ê¸° ì¶•ì†Œ ì‹¤íŒ¨, ë²¡í„° ì œì™¸: {vector['id']}")
                        
            except Exception as exc:
                logger.error(f"ë©”íƒ€ë°ì´í„° í¬ê¸° í™•ì¸ ì‹¤íŒ¨: {vector['id']}, ì˜¤ë¥˜: {exc}")
                skipped_count += 1
        
        if skipped_count > 0:
            logger.warning(f"ë©”íƒ€ë°ì´í„° í¬ê¸° ì œí•œìœ¼ë¡œ ì¸í•´ {skipped_count}ê°œ ë²¡í„°ê°€ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        logger.info(f"ë©”íƒ€ë°ì´í„° í¬ê¸° í•„í„°ë§ ì™„ë£Œ: {len(filtered_vectors)}/{len(vectors)}ê°œ ë²¡í„° ìœ ì§€")
        return filtered_vectors
    
    def _reduce_metadata_size(self, vector: Dict[str, Any], max_size: int) -> Optional[Dict[str, Any]]:
        """
        ë©”íƒ€ë°ì´í„° í¬ê¸°ë¥¼ ì¤„ì´ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ë¥¼ ì¶•ì†Œí•©ë‹ˆë‹¤.
        
        Args:
            vector: ì¶•ì†Œí•  ë²¡í„°
            max_size: ìµœëŒ€ ë©”íƒ€ë°ì´í„° í¬ê¸°
            
        Returns:
            ì¶•ì†Œëœ ë²¡í„° ë˜ëŠ” None (ì¶•ì†Œ ì‹¤íŒ¨ ì‹œ)
        """
        import json
        
        # í…ìŠ¤íŠ¸ í•„ë“œë“¤ì„ ì¶•ì†Œí•  ìˆ˜ ìˆëŠ” ìˆœì„œ (ì¤‘ìš”ë„ ìˆœ)
        text_fields = ['assistant_text', 'text']
        
        for field in text_fields:
            if field not in vector['metadata']:
                continue
                
            original_text = vector['metadata'][field]
            if not original_text:
                continue
            
            # í…ìŠ¤íŠ¸ë¥¼ 50%ì”© ì¤„ì—¬ê°€ë©´ì„œ ì‹œë„
            reduction_ratio = 0.5
            max_attempts = 5
            
            for attempt in range(max_attempts):
                # í…ìŠ¤íŠ¸ ì¶•ì†Œ
                reduced_text = original_text[:int(len(original_text) * (reduction_ratio ** attempt))]
                
                # ì¶•ì†Œëœ í…ìŠ¤íŠ¸ë¡œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                test_metadata = vector['metadata'].copy()
                test_metadata[field] = reduced_text
                
                # í¬ê¸° í™•ì¸
                metadata_json = json.dumps(test_metadata, ensure_ascii=False)
                metadata_size = len(metadata_json.encode('utf-8'))
                
                if metadata_size <= max_size:
                    # í¬ê¸° ì œí•œì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°
                    reduced_vector = vector.copy()
                    reduced_vector['metadata'] = test_metadata
                    logger.info(f"í…ìŠ¤íŠ¸ ì¶•ì†Œ ì„±ê³µ: {field} {len(original_text)} -> {len(reduced_text)} ë¬¸ì")
                    return reduced_vector
                
                # ë§ˆì§€ë§‰ ì‹œë„ì—ì„œë„ ì‹¤íŒ¨í•œ ê²½ìš°
                if attempt == max_attempts - 1:
                    logger.warning(f"í…ìŠ¤íŠ¸ ì¶•ì†Œ ì‹¤íŒ¨: {field} í•„ë“œ")
                    return None
        
        return None
    
    async def embed_and_upsert_to_pinecone(self, chunks: List[Dict[str, Any]], user_id: str) -> bool:
        """
        User ë°œí™”ë§Œ ì„ë² ë”©í•˜ì—¬ Pineconeì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
        ì‚¬ìš©ìë³„ namespaceë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ê²©ë¦¬í•©ë‹ˆë‹¤.
        ë©”íƒ€ë°ì´í„° í¬ê¸° ì œí•œ(40KB)ì„ ì´ˆê³¼í•  ê²½ìš° ëŒ€í™” ë‚´ì—­ì„ ì¤„ì—¬ê°€ë©´ì„œ ì¬ì‹œë„í•©ë‹ˆë‹¤.
        
        Args:
            chunks: ì²˜ë¦¬í•  ì²­í¬ ë¦¬ìŠ¤íŠ¸
            user_id: ì‚¬ìš©ì ID (namespaceë¡œ ì‚¬ìš©)
            
        Returns:
            ì—…ë¡œë“œ ì„±ê³µ ì—¬ë¶€
            
        Raises:
            ConversationRAGServiceError: ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ
        """
        try:
            logger.info(f"Pinecone ì„ë² ë”© ë° ì—…ë¡œë“œ ì‹œì‘: user_id={user_id}")
            
            # User ë°œí™”ë§Œ í•„í„°ë§
            user_chunks = [chunk for chunk in chunks if chunk['role'] == 'user']
            
            if not user_chunks:
                logger.warning("User ë°œí™”ê°€ ì—†ì–´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return True
            
            logger.info(f"User ë°œí™” {len(user_chunks)}ê°œ ì„ë² ë”© ì‹œì‘")
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            texts = [chunk['text'] for chunk in user_chunks]
            
            # ì„ë² ë”© ìƒì„±
            embeddings = await self.cohere_service.embed_texts(
                texts,
                model=self.cohere_service._default_model,
                input_type="search_document"
            )
            
            if not embeddings:
                raise ConversationRAGServiceError("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            
            # Pinecone ì—…ë¡œë“œ ë°ì´í„° ì¤€ë¹„ (User ë°œí™” + Assistant ë‹µë³€ í¬í•¨)
            vectors = []
            for i, (chunk, embedding) in enumerate(zip(user_chunks, embeddings)):
                # Assistant ë‹µë³€ ì°¾ê¸° (ì´ì „ ì²­í¬ê°€ Assistantì¸ ê²½ìš°)
                assistant_text = ""
                current_index = chunk['chunk_index']
                prev_index = current_index - 1
                
                # ì´ì „ ì²­í¬ê°€ Assistantì¸ì§€ í™•ì¸
                for all_chunk in chunks:
                    if (all_chunk['chunk_index'] == prev_index and 
                        all_chunk['role'] == 'assistant'):
                        assistant_text = all_chunk['text']
                        break
                
                vector_data = {
                    'id': chunk['chunk_id'],
                    'values': embedding,
                    'metadata': {
                        'text': chunk['text'],  # User ë°œí™”
                        'assistant_text': assistant_text,  # Assistant ë‹µë³€
                        'document_id': chunk['document_id'],
                        'conversation_id': chunk['conversation_id'],
                        'conversation_title': chunk['conversation_title'],
                        'timestamp': chunk['timestamp'],
                        'role': chunk['role']
                    }
                }
                vectors.append(vector_data)
            
            # ë©”íƒ€ë°ì´í„° í¬ê¸° ì œí•œ ì²˜ë¦¬
            vectors = self._filter_vectors_by_metadata_size(vectors, user_id)
            
            if not vectors:
                logger.warning("ë©”íƒ€ë°ì´í„° í¬ê¸° ì œí•œìœ¼ë¡œ ì¸í•´ ì—…ë¡œë“œí•  ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
            
            # Pineconeì— ì‚¬ìš©ìë³„ namespaceë¡œ ë°°ì¹˜ ì—…ë¡œë“œ (100ê°œì”©)
            batch_size = 100
            total_vectors = len(vectors)
            uploaded_count = 0
            
            for i in range(0, total_vectors, batch_size):
                batch_vectors = vectors[i:i + batch_size]
                batch_num = (i // batch_size) + 1
                total_batches = (total_vectors + batch_size - 1) // batch_size
                
                logger.info(f"Pinecone ë°°ì¹˜ {batch_num}/{total_batches} ì—…ë¡œë“œ ì¤‘: {len(batch_vectors)}ê°œ ë²¡í„°")
                
                # ë°°ì¹˜ ì—…ë¡œë“œ (user_idë¥¼ namespaceë¡œ ì‚¬ìš©)
                self.pinecone_service.upsert_vectors(batch_vectors, namespace=user_id)
                uploaded_count += len(batch_vectors)
                
                logger.info(f"Pinecone ë°°ì¹˜ {batch_num} ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_count}/{total_vectors}ê°œ ë²¡í„° ì—…ë¡œë“œë¨")
            
            logger.info(f"Pinecone ì—…ë¡œë“œ ì™„ë£Œ: ì´ {uploaded_count}ê°œ ë²¡í„° (namespace: {user_id})")
            return True
            
        except Exception as exc:
            logger.error(f"Pinecone ì„ë² ë”© ë° ì—…ë¡œë“œ ì‹¤íŒ¨: {exc}")
            raise ConversationRAGServiceError(f"Pinecone ì„ë² ë”© ë° ì—…ë¡œë“œ ì‹¤íŒ¨: {exc}") from exc
    
    async def get_rag_context(self, query: str, user_id: str, top_k: int = 5) -> str:
        """
        RAG ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            user_id: ì‚¬ìš©ì ID
            top_k: ê²€ìƒ‰í•  ìœ ì‚¬í•œ ë°œí™” ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
            
        Returns:
            ì¡°í•©ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ë‹µë³€ â†’ ì§ˆë¬¸ ìˆœì„œ)
            
        Raises:
            ConversationRAGServiceError: ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ
        """
        try:
            logger.info(f"ğŸ” RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œì‘")
            logger.info(f"   ğŸ“ query: '{query}'")
            logger.info(f"   ğŸ‘¤ user_id: '{user_id}'")
            logger.info(f"   ğŸ”¢ top_k: {top_k}")
            
            # 1. ê²€ìƒ‰ ë‹¨ê³„: ì¿¼ë¦¬ ì„ë² ë”© ë° ìœ ì‚¬ë„ ê²€ìƒ‰
            logger.info(f"ğŸ“¤ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹œì‘")
            logger.info(f"   ğŸ”— cohere_service.embed_texts í˜¸ì¶œ")
            logger.info(f"   ğŸ“‹ ëª¨ë¸: embed-multilingual-v3.0")
            logger.info(f"   ğŸ“‹ ì…ë ¥ íƒ€ì…: search_query")
            
            query_embeddings = await self.cohere_service.embed_texts(
                [query],
                model="embed-multilingual-v3.0",
                input_type="search_query"
            )
            
            logger.info(f"ğŸ“¥ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
            logger.info(f"   ğŸ“Š ì„ë² ë”© ìˆ˜: {len(query_embeddings) if query_embeddings else 0}")
            logger.info(f"   ğŸ“Š ì„ë² ë”© ì°¨ì›: {len(query_embeddings[0]) if query_embeddings and query_embeddings[0] else 0}")
            
            if not query_embeddings:
                logger.error(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                raise ConversationRAGServiceError("ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            
            # Pineconeì—ì„œ í•´ë‹¹ ì‚¬ìš©ìì˜ User ë°œí™”ë§Œ ê²€ìƒ‰ (user_id namespace ì‚¬ìš©)
            logger.info(f"ğŸ“¤ Pinecone ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œì‘")
            logger.info(f"   ğŸ”— pinecone_service.query_similar í˜¸ì¶œ")
            logger.info(f"   ğŸ“‹ top_k: {top_k}")
            logger.info(f"   ğŸ“‹ include_metadata: True")
            logger.info(f"   ğŸ“‹ namespace: {user_id}")
            
            search_response = self.pinecone_service.query_similar(
                query_embeddings[0],
                top_k=top_k,
                include_metadata=True,
                namespace=user_id  # user_idë¥¼ namespaceë¡œ ì‚¬ìš©
            )
            
            logger.info(f"ğŸ“¥ Pinecone ê²€ìƒ‰ ì™„ë£Œ")
            logger.info(f"   ğŸ“Š ê²€ìƒ‰ ì‘ë‹µ: {search_response}")
            
            matches = search_response.get('matches', [])
            if not matches:
                logger.warning("í•´ë‹¹ ì‚¬ìš©ìì˜ ëŒ€í™”ì—ì„œ ìœ ì‚¬í•œ ë°œí™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return ""
            
            # ëª¨ë“  ë§¤ì¹˜ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
            context_parts = []
            
            for i, match in enumerate(matches):
                metadata = match.get('metadata', {})
                logger.info(f"ìœ ì‚¬í•œ User ì²­í¬ {i+1} ë°œê²¬: {match['id']}")
                
                # Assistant ë‹µë³€ ì¶”ê°€ (ë¨¼ì €) - Pinecone ë©”íƒ€ë°ì´í„°ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                assistant_text = metadata.get('assistant_text', '')
                if assistant_text:
                    context_parts.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {assistant_text}")
                
                # User ë°œí™” ì¶”ê°€ (ë‚˜ì¤‘ì—) - Pinecone ë©”íƒ€ë°ì´í„°ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                user_text = metadata.get('text', '')
                if user_text:
                    context_parts.append(f"ì‚¬ìš©ì: {user_text}")
                
                # ê° ëŒ€í™” ì‚¬ì´ì— êµ¬ë¶„ì„  ì¶”ê°€
                if i < len(matches) - 1:
                    context_parts.append("---")
            
            # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ì¡°í•© (ë‹µë³€ â†’ ì§ˆë¬¸ ìˆœì„œ)
            final_context = "\n\n".join(context_parts)
            
            logger.info(f"RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì™„ë£Œ: {len(final_context)}ì")
            return final_context
            
        except Exception as exc:
            logger.error(f"RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {exc}")
            raise ConversationRAGServiceError(f"RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {exc}") from exc


# í¸ì˜ í•¨ìˆ˜ë“¤
def process_conversation_json(user_id: str, document_id: str) -> List[Dict[str, Any]]:
    """Firebase Storageì—ì„œ ëŒ€í™” JSON íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì²­í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    service = ConversationRAGService()
    return service.process_conversation_json(user_id, document_id)




async def embed_and_upsert_to_pinecone(chunks: List[Dict[str, Any]], user_id: str) -> bool:
    """User ë°œí™”ë¥¼ ì„ë² ë”©í•˜ì—¬ Pineconeì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    service = ConversationRAGService()
    return await service.embed_and_upsert_to_pinecone(chunks, user_id)


async def get_rag_context(query: str, user_id: str, top_k: int = 5) -> str:
    """RAG ê²€ìƒ‰ì„ í†µí•´ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    service = ConversationRAGService()
    return await service.get_rag_context(query, user_id, top_k)